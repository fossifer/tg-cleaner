import shap
import jieba
import pandas as pd
import numpy as np
import joblib
import matplotlib.pyplot as plt
from glob import glob
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.model_selection import train_test_split
from sklearn.metrics import precision_score, recall_score, f1_score, classification_report

seed = 42

# Set a font that supports Chinese characters
plt.rcParams['font.sans-serif'] = ['PingFang SC', 'STHeiti', 'Heiti TC']
plt.rcParams['axes.unicode_minus'] = False  # Correctly display negative signs

class TelegramMessageClassifier:
    def __init__(self):
        self.vectorizer = CountVectorizer(tokenizer=self.tokenize)
        self.classifier = MultinomialNB()
    
    def tokenize(self, message):
        tokens = list(jieba.cut(message, cut_all=False))
        return tokens

    def preprocess_messages(self, messages):
        return [' '.join(self.tokenize(message)) for message in messages]
    
    def load(self, path):
        self.vectorizer, self.classifier = joblib.load(path)

    def train(self, messages, labels, output='models/nb_model_and_vectorizer.joblib'):
        """
        è®­ç»ƒåˆ†ç±»å™¨
        
        :param messages: æ˜µç§°åˆ—è¡¨
        :param labels: å¯¹åº”çš„æ ‡ç­¾ï¼ˆspam/hamï¼‰
        """
        # é¢„å¤„ç†æ˜µç§°
        processed_messages = self.preprocess_messages(messages)
        
        # å°†æ•°æ®åˆ†ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†
        X_train, X_test, y_train, y_test = train_test_split(
            processed_messages, labels, test_size=0.2, random_state=seed
        )
        
        # ç‰¹å¾å‘é‡åŒ–
        X_train_vectorized = self.vectorizer.fit_transform(X_train)
        X_test_vectorized = self.vectorizer.transform(X_test)
        
        # è®­ç»ƒåˆ†ç±»å™¨
        self.classifier.fit(X_train_vectorized, y_train)
        
        # åœ¨æµ‹è¯•é›†ä¸Šè¿›è¡Œé¢„æµ‹
        y_pred = self.classifier.predict(X_test_vectorized)
        
        # è¯„ä¼°æ¨¡å‹
        train_accuracy = self.classifier.score(X_train_vectorized, y_train)
        test_accuracy = self.classifier.score(X_test_vectorized, y_test)
        
        print(f"è®­ç»ƒé›†å‡†ç¡®ç‡: {train_accuracy:.2%}")
        print(f"æµ‹è¯•é›†å‡†ç¡®ç‡: {test_accuracy:.2%}")

        # è®¡ç®—ç²¾ç¡®ç‡ã€å¬å›ç‡å’ŒF1åˆ†æ•°
        precision = precision_score(y_test, y_pred, pos_label=True)
        recall = recall_score(y_test, y_pred, pos_label=True)
        f1 = f1_score(y_test, y_pred, pos_label=True)
        
        # æ‰“å°è¯¦ç»†çš„åˆ†ç±»æŠ¥å‘Š
        print("åˆ†ç±»æŠ¥å‘Š:")
        print(classification_report(y_test, y_pred))
        print(f"Spamç²¾ç¡®ç‡: {precision:.2%}")
        print(f"Spamå¬å›ç‡: {recall:.2%}")
        print(f"Spam F1åˆ†æ•°: {f1:.2%}")

        # åˆ†æé”™è¯¯åˆ†ç±»æ ·æœ¬
        print("\né”™è¯¯åˆ†ç±»æ ·æœ¬åˆ†æ:")
        misclassified_indices = np.where(y_test != y_pred)[0]

        # é™åˆ¶æ‰“å°å‰20ä¸ªé”™è¯¯åˆ†ç±»æ ·æœ¬
        print(f"å…±æœ‰ {len(misclassified_indices)} ä¸ªé”™è¯¯åˆ†ç±»æ ·æœ¬")
        print("å‰20ä¸ªé”™è¯¯åˆ†ç±»æ ·æœ¬ï¼š")
        for idx in misclassified_indices[20:60]:
            true_label = y_test[idx]
            pred_label = y_pred[idx]
            nickname = X_test[idx]
            prob_spam = self.classifier.predict_proba(X_test_vectorized[idx])[0][1]

            print(f"æ˜µç§°: {nickname}")
            print(f"çœŸå®æ ‡ç­¾: {true_label}")
            print(f"é¢„æµ‹æ ‡ç­¾: {pred_label}")
            print(f"Spamæ¦‚ç‡: {prob_spam:.2%}")
            print("-" * 30)

        joblib.dump((self.vectorizer, self.classifier), output)
    
    def predict(self, messages):
        """
        é¢„æµ‹æ˜µç§°æ˜¯å¦ä¸ºspam
        
        :param messages: å¾…é¢„æµ‹çš„æ˜µç§°åˆ—è¡¨
        :return: é¢„æµ‹ç»“æœï¼ˆspam/hamï¼‰
        """
        processed_messages = self.preprocess_messages(messages)
        processed_vectorized = self.vectorizer.transform(processed_messages)
        predictions = self.classifier.predict(processed_vectorized)
        return predictions
    
    def predict_proba(self, messages):
        """
        è·å–spamçš„æ¦‚ç‡
        
        :param messages: å¾…é¢„æµ‹çš„æ˜µç§°åˆ—è¡¨
        :return: spamæ¦‚ç‡
        """
        processed_messages = self.preprocess_messages(messages)
        processed_vectorized = self.vectorizer.transform(processed_messages)
        spam_probabilities = self.classifier.predict_proba(processed_vectorized)[:, 1]
        return spam_probabilities
    
    def get_vectorized(self, X_raw):
        X_processed = self.preprocess_messages(X_raw)
        return self.vectorizer.transform(X_processed)

def normalize_value(value):
    if isinstance(value, bool):
        return value
    elif isinstance(value, (int, float)):
        return bool(value)
    elif isinstance(value, str):
        lower_value = value.lower()
        if lower_value in ('true', 't', '1', 'yes', 'y'):
            return True
        else:
            return False
    else:
        return False

def main():
    # è¯»å–CSVæ•°æ®
    df_main = pd.read_csv('data/messages.csv')
    # Fill missing values
    df_main.fillna({'nickname': '__MISSING__'}, inplace=True)

    # Load all exported CSVs
    export_files = glob('data/export/*.csv')
    export_dfs = []

    for file in export_files:
        df_export = pd.read_csv(file)
        df_export.fillna({'nickname': '__MISSING__'}, inplace=True)

        # Normalize is_spam
        df_export['is_spam'] = df_export['is_spam'].apply(normalize_value)
        export_dfs.append(df_export)
        # Normalize col name
        df_export['content'] = df_export['message_text']

    df = pd.concat([df_main] + export_dfs, ignore_index=True)

    # Concat input string fields
    df['input'] = df['nickname'] + " [SEP] " + df['content']
    
    # å‡†å¤‡æ•°æ®
    messages = df['input'].tolist()
    labels = df['is_spam'].tolist()

    # åˆå§‹åŒ–åˆ†ç±»å™¨
    classifier = TelegramMessageClassifier()
    
    # è®­ç»ƒåˆ†ç±»å™¨
    # classifier.train(messages, labels, output='models/nb_20250502.joblib')
    
    classifier.load('models/nb_20250502.joblib')

    # é¢„æµ‹ç¤ºä¾‹
    test_messages = [
        "Yvonne Watson [SEP] 6å•Š   å“ªä¸ªè·¯å­æœ‰ä»–é‡     @wikipedia       ç‰›é€¼ ä¸€å°æ—¶æä¸‰ä¸‡ æˆ‘åœ¨å¤–é¢æ¬ç –éƒ½æ²¡æœ‰é‚£ä¹ˆå¤š çœŸç‰›é€¼",  # spam
        'å›´æ£‹æ¾œæ¾œ [SEP] è¿˜æœ‰ä¸€ä¸ªæœˆè¿‡å¹´äº†ï¼Œè¿˜åœ¨æ‰¾é¡¹æ…•çš„å¯ä»¥è¿‡æ¥ç§ç§æˆ‘è¿™é¡¹æ…•ï¼Œé—®é—®ç»ˆç©¶æ˜¯å¥½çš„ï¼Œæœ‰æ—¶é—´å¾—æ¥    @wikibooks',  # spam
        'e [SEP] é˜¿é“å¤«çš„è‚¡ç¥¨è·Œäº†æ°”æ€¥è´¥åäº†\næ¯•ç«Ÿè¾‰è¾¾å¤§è‚¡ä¸œ',  # ham
        'C World [SEP] https://github.com/Alex313031/thorium/issues/307',  # ham
        'aaa llll-æ•£ä¿®ã‚ã‚‹ [SEP] ã€è‡ªåŠ¨é©¾é©¶VSè¾…åŠ©é©¾é©¶ã€ç¦»è°±åŠ¨ç”»ã€ŠæŸ´æ¥äº†ã€‹ã€‘-å“”å“©å“”å“©ã€‘ https://b23.tv/XL8TgQd',  # ham
        'K_M101918 [SEP] æœ‰çˆ¸çˆ¸ç¾è¾±è°ƒæ•™æˆ‘å—ï¼Ÿæ¹¿äº†æƒ³è¦äº†ğŸ˜‘',  # spam
        'grisha_in919 [SEP] æœ‰æ²¡ç¡çš„æ­å­äº¤æµèŠæ¶©æ¶©å— æƒ³æ¹¿äº†ğŸ«¥',  # spam
        'ISSAM Ucjgoho [SEP] æ—¥ ä¾› åº” ä¸Š ä¸‡ ç¬” è½¬ è´¦ èƒ½ é‡ è®¤ å‡† æœº å™¨ äºº ID ï¼š @wikipedia_zh %2F4,L/34zY(9g',  # spam
        'dhjdjjde [SEP] æ‰§è¡ŒåŠ›å¼ºçš„æ¥ ä¸€å¤©5o o o. @yuchen37408',  # spam
        'ME [SEP] ä¸»æµ å±±å¯¨ ç¾è²¨ ğŸ‘ˆğŸ‘ˆ  çœ‹è¡Œæƒ…äº¤æµè¿›ğŸ«°ğŸ«°',   # spam
        'Lucky Cat [SEP] ç®€å•æ“ä½œè·Ÿæˆ‘è¿‡å¥½æ—¥å­   @Gooblaj2',  # spam
        'å¤ªæœ‰é’±äº† å‘äº†1288uğŸ‘‰ @kofunt',  # spam
        'å˜€æ»´wzyz22 Agbomon [SEP] æœºä¼šå°±åœ¨çœ¼å‰-ä¸»é¡µäº†è§£',  # spam
        'å˜€æ»´ynyz22 Delbole [SEP] å‡ å¤©èµšäº†å‡ ä¸‡ ä¸€ä¸ªæœˆå¯ä»¥å¥”é©°c æŠ“ç´§æ—¶é—´.åšçš„ä¸»å¤œ',  # spam
        'å˜€æ»´mqyz22 Chimboya [SEP] æ²¡é¡¹ç›®åšï¼Ÿæˆ‘ä»¬æä¾›è½»æ¾çš„èµšé’±æ–¹å¼,ç«¹é¡µç®€ä»‹',  # spam
        'å¥½ pppp222060999999 [SEP] é¹°å³°åŒå­¦',  # ham
        'å¥½ pppp222060999999 [SEP] å¤§å®¶å¥½',  # ham
        'å¥½ pppp222060999999 [SEP] é£å£æ–°é¡¹ç›®çº¯ç»¿è‰²æ‹›äººï¼ŒåŠ¨åŠ¨æ‰‹ä¸€å¤©ä¿åº•9kçœ‹æˆ‘ç­¾!é“­',  # spam
        'æ´¾å°æ˜Ÿ [SEP] å¥½æƒ³è¦ï¼æœ‰å–œæ¬¢å°ç†Ÿå¥³çš„å—ï¼Ÿå‹ƒèµ·çš„æœ‰å—ğŸ¤£',  # spam
        'Erika [SEP] ä»€ä¹ˆâ‰  æƒ³è¦æ’¸æ²¡å¥½ç‰‡ï¼Œæ¥è¿™é‡Œå§  æ”¯æŒå‘å›¾æœç´¢å¤§ç‰‡  @seseyibot  ï¼ˆå…¨ç½‘æœ€é½å…¨ï¼‰',  # spam
        'HOSSAIN MD SAZZAD [SEP] I want to Learn Biotechnology',  # ham
        'è¯—è¯—y [SEP] å¤±çœ ï¼Œæœ‰èŠçš„å—ï¼Ÿå«æˆ‘å…¬ä¸»ç»™çœ‹å†…æ­ğŸ•º',  # spam
        'M [SEP] æˆ‘æ“ä»–å¦ˆçš„ï¼Œé£æœºä¸Šéª—å­æ€ä¹ˆè¿™ä¹ˆå¤šï¼ç›´åˆ°æˆ‘é‡è§ä»– @WWW_USDT699 æ‰çŸ¥é“ä¸€ä¸ªå°æ—¶2W æ˜¯å¤šä¹ˆç®€å•ï¼Œç°åœ¨æ·±åœ³éƒ½å·²ç»ä¹°äº†ä¸€å¥—æˆ¿äº†',  # spam
        "å¶ æ« [SEP] githubæŒ‚ä¸ªæ¢¯å­å§",  # ham
        "é’“é±¼ [SEP] ä½ è§è¿‡è¿™æ ·çš„äººå—ï¼Ÿ @www_usdt699 æ˜¯å¦‚ä½•åœ¨ä¸€å¤©å†…å®Œæˆä¸€è¾†å¥”é©°ä»·å€¼äº¤æ˜“çš„ï¼Ÿè¿™ç®€ç›´æ˜¯å¤ªéœ‡æ’¼äº†ï¼",  # spam
    ]
    predictions = classifier.predict(test_messages)
    probabilities = classifier.predict_proba(test_messages)
    
    for nickname, pred, prob in zip(test_messages, predictions, probabilities):
        print(f"æ˜µç§°: {nickname}, é¢„æµ‹: {pred}, Spamæ¦‚ç‡: {prob:.2%}")

    return

    messages_to_explain = [
        "å¶ æ« [SEP] githubæŒ‚ä¸ªæ¢¯å­å§",
    ]
    # è·å–è¦è§£é‡Šçš„æ ·æœ¬çš„å‘é‡åŒ–ç‰¹å¾
    X_to_explain = classifier.get_vectorized(messages_to_explain).toarray()
    
    # åˆ›å»ºä¸€ä¸ªå¯è°ƒç”¨çš„æ¨¡å‹åŒ…è£…å‡½æ•°
    def model_predict(x):
        return classifier.classifier.predict_proba(x)[:, 1]  # è¿”å›æ­£ç±»ï¼ˆspamï¼‰çš„æ¦‚ç‡
    
    # è·å–èƒŒæ™¯æ•°æ®
    processed_messages = classifier.preprocess_messages(messages)
    X_train, _, _, _ = train_test_split(
        processed_messages, labels, test_size=0.2, random_state=seed
    )
    X_background = classifier.vectorizer.transform(X_train).toarray()
    
    # ä½¿ç”¨KernelExplainerï¼Œå®ƒé€‚ç”¨äºä»»ä½•æ¨¡å‹
    explainer = shap.KernelExplainer(
        model_predict, 
        shap.sample(X_background, 100),  # ä½¿ç”¨æ ·æœ¬ä½œä¸ºèƒŒæ™¯æ•°æ®ä»¥æé«˜æ•ˆç‡
        feature_names=classifier.vectorizer.get_feature_names_out()
    )
    
    # è®¡ç®—SHAPå€¼
    shap_values = explainer.shap_values(X_to_explain)
    
    # æ˜¾ç¤ºwaterfallå›¾
    plt.figure(figsize=(12, 8))
    shap.plots.waterfall(shap.Explanation(
        values=shap_values[0], 
        base_values=explainer.expected_value,
        data=X_to_explain[0],
        feature_names=classifier.vectorizer.get_feature_names_out()
    ), max_display=20)
    plt.tight_layout()
    # plt.savefig('shap_waterfall.png')
    plt.close()
    
    # æ˜¾ç¤ºå†³ç­–å›¾
    plt.figure(figsize=(12, 8))
    shap.decision_plot(
        explainer.expected_value, 
        shap_values[0], 
        X_to_explain[0],
        feature_names=classifier.vectorizer.get_feature_names_out()
    )
    plt.tight_layout()
    # plt.savefig('shap_decision_plot.png')
    plt.close()
    
    # æ‰“å°é¢„æµ‹ç»“æœ
    pred = classifier.predict(messages_to_explain)
    prob = classifier.predict_proba(messages_to_explain)
    print(f"æ¶ˆæ¯: {messages_to_explain[0]}")
    print(f"é¢„æµ‹: {'Spam' if pred[0] else 'Ham'}, æ¦‚ç‡: {prob[0]:.2%}")
    print(f"SHAPè§£é‡Šå·²ä¿å­˜ä¸ºå›¾ç‰‡æ–‡ä»¶")

if __name__ == "__main__":
    main()
